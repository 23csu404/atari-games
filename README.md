ğŸ® Breakout Game â€“ Reinforcement Learning Agent

A Q-Learning Based Autonomous Game Player

ğŸ“Œ Overview

This project implements a Reinforcement Learning (RL) agent that learns to play the classic Breakout arcade game autonomously.
The environment is built using Python + Pygame, while the agent is trained using the Q-Learning algorithm.
The agent observes the environment, takes actions, receives rewards, and gradually learns to hit bricks and avoid losing the ball.

This repository includes:

Custom Pygame Breakout environment

Q-Learning agent

State discretization

Reward function design

Training loop & performance graphs

Autonomous gameplay mode

Q-table saving & reloading

âœ¨ Features

ğŸ§  Custom RL Agent (Q-Learning)

ğŸ® Smooth Breakout Environment using Pygame

â­ Improved paddle movement with smoothing

ğŸ“Š Training visualizations (Rewards, Bricks, Lives)

ğŸ“ Modular code structure (Environment, Agent, Training, Gameplay)

ğŸ’¾ Q-table persistence (q_table.pkl)

ğŸš€ Autonomous gameplay demonstration

ğŸ“‚ Project Structure
â”œâ”€â”€ environment.py        # Breakout game environment (Pygame)
â”œâ”€â”€ agent.py              # Q-learning agent implementation
â”œâ”€â”€ train.py              # Training script
â”œâ”€â”€ play.py               # Run autonomous gameplay with trained agent
â”œâ”€â”€ graphs/               # Training visualizations
â”œâ”€â”€ q_table.pkl           # Saved Q-table after training
â””â”€â”€ README.md             # Project documentation

ğŸ•¹ï¸ Game Environment Design

The custom environment includes:

Paddle

Moves left/right

Smooth movement reduces jitter

Controlled entirely by agent

Ball

Continuous physics

Collisions with paddle, bricks, walls

Bricks

Multiple rows

Destroyed upon collision

Provide reward

State Representation (Discrete):
(ball_x_bin, ball_y_bin, ball_dx_bin, ball_dy_bin, paddle_x_bin)

Action Space
Action	Meaning
0	Stay
1	Move Left
2	Move Right
Reward Structure

+10 â†’ Hit a brick

+2 â†’ Ball hits paddle

âˆ’20 â†’ Lose ball (life lost)

+50 â†’ Clear all bricks

Optional small negative reward for delaying the game

ğŸ¤– Q-Learning Algorithm

The agent updates its Q-values using the Bellman equation:

Q(s, a) = Q(s, a) + Î± [ r + Î³ max(Q(s', :)) â€“ Q(s, a) ]


âœ” Off-policy
âœ” Fast for discrete states
âœ” Works well for medium-sized RL tasks like Breakout

ğŸ“ˆ Training Process

Training involves:

Reset environment

Choose action (epsilon-greedy)

Observe reward + next state

Update Q-table

Decay exploration

Save q_table.pkl

Performance graphs include:

Average reward per episode

Bricks destroyed

Lives remaining

Paddle-ball interaction quality

ğŸ¥ Autonomous Gameplay

Once trained, run:

python play.py


The agent will:

Track the ball accurately

Move the paddle smoothly

Destroy bricks efficiently

Play the full game with no human input

ğŸ§ª Challenges & Solutions
âŒ Paddle jitter

âœ” Added smoothing + better discretization

âŒ Q-table not converging

âœ” Tuned learning rate, gamma, reward structure

âŒ Poor collision detection

âœ” Improved bounce logic + angle handling

âŒ Agent stuck in loops

âœ” Modified exploration & reward shaping
